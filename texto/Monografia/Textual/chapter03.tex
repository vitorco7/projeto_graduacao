\chapter{Metodologia Proposta}
\label{chap3}

No capítulo anterior, foram apresentados os conceitos e as tecnologias relevantes para a compreensão da solução de monitoramento desenvolvida. Contudo, vale ressaltar que nem todas as ferramentas descritas foram efetivamente empregadas na implementação final da proposta.

Este capítulo tem por objetivo detalhar todo o processo de desenvolvimento da solução de monitoramento, desde as primeiras considerações e escolhas de arquitetura, passando pela seleção e adaptação das ferramentas adotadas, até a apresentação da solução final implementada. Serão discutidas as motivações para determinadas decisões técnicas, desafios encontrados ao longo do desenvolvimento, e os métodos empregados para contorná-los, buscando sempre respaldar as opções feitas com base nos conceitos apresentados no Capítulo 2. Também são descritos os equipamentos utilizados durante o desenvolvimento e todo o código fonte do projeto está versionado no repositório Git \citep{vitorcossetti2025}.

As especificações do sistema operacional serão apresentadas posteriormente, visto que a escolha desse aspecto evoluiu ao longo do desenvolvimento, conforme será detalhado neste capítulo.

\section{Abordagem Preliminar}
\label{section:AbordagemPreliminar}

Esta seção apresenta uma visão geral das decisões iniciais tomadas antes da definição da arquitetura final da solução de monitoramento. As primeiras etapas deste trabalho podem ser divididas em dois momentos: inicialmente, o escopo era voltado para o monitoramento de infraestruturas de TI tradicionais (como servidores e clusters); em seguida, evoluiu para englobar também dispositivos conectados à rede de forma abrangente, sem restrições de tipagem.

\subsection{Escopo inicial - Infraestrutura Tradicional}
\label{subsection:EscopoInicial}

Durante as discussões preliminares, o objetivo era monitorar servidores. Foi considerada uma arquitetura onde um dispositivo --- podendo ser um NUC ou Raspberry Pi com softwares instalados --- realizaria a coleta de métricas deste servidor e, após o processamento destes dados, disponibilizaria-os em painéis com visualizações gráficas especializadas. Alternativas com Orange Pi como equipamento foram analisadas, mas a escolha final recaiu sobre o NUC ou Raspberry Pi devido à suas respectivas popularidades e suporte.

Entretanto, como não foi possível adquirir fisicamente um NUC ou Raspberry Pi, decidiu-se simular toda a \foreign{stack} de monitoramento em uma máquina virtual. Com isso, a possibilidade da utilização de distribuições ou versões do Raspberry Pi OS para fins de desenvolvimento foi descartada, por tratar-se de um sistema operacional baseado em arquitetura ARM, incompatível com a arquitetura x86-64 dos equipamentos disponíveis.

No \foreign{desktop} disponível, à época com sistema operacional não-Linux, instalou-se o VMWare Workstation Player (versão gratuita do VMWare Workstation) para configuração de uma VM com Rocky Linux. A escolha do deste SO\abbrev{SO}{Sistema Operacional} baseou-se em sua compatibilidade com o CentOS, estabilidade e longo ciclo de suporte, características valorizadas em ambientes corporativos.

No entanto, o VMWare Workstation Player mostrou-se limitado em recursos, dificultando a execução de múltiplas VMs. Por isso, migrou-se para o VirtualBox, o que trouxe maior flexibilidade e controle.

Esta estratégia inicial apresentou limitações importantes: a primeira era a maior dificuldade na aquisição de dados, pois seria necessário obter um servidor físico para coleta dos mesmos. Outra limitação foi o elevado custo computacional inerente ao uso de VMs em comparação a soluções baseadas em contêineres, e pouca modularidade, resultando em manutenção complexa e baixa praticidade.

Devido a esses fatores, verificou-se a necessidade de reavaliar o escopo e buscar abordagens mais aderentes aos recursos disponíveis e aos objetivos do projeto.

\begin{figure}[H]
\centering
\includegraphics[scale=1]{Imagens/chap03/v0xv1_stack.pdf}
\caption{Da esquerda para a direita - \foreign{stack} conceitual, \foreign{stack} implementada.}
\label{fig:StackImplementada}
\end{figure}

\begin{figure}[H]
\centering
\setlength{\abovecaptionskip}{-20pt}
\includegraphics[width=\textwidth]{Imagens/chap03/v0_diagram.pdf}
\caption{Arquitetura conceitual.}
\label{fig:ArquiteturaConceitual}
\end{figure}

\begin{figure}[H]
\centering
\setlength{\abovecaptionskip}{-20pt}
\includegraphics[width=\textwidth]{Imagens/chap03/v1_diagram.pdf}
\caption{Arquitetura implementada.}
\label{fig:ArquiteturaImplementada}
\end{figure}

\subsection{Escopo Reformulado - Monitoramento Amplo}
\label{subsection:EscopoReformulado}

Até então, os sistemas operacionais dos \foreign{hosts} disponíveis eram distintos (o \foreign{desktop} com um sistema não-Linux e o \foreign{notebook} com Ubuntu Desktop). Considerando as limitações encontradas na abordagem inicial, optou-se por instalar o Ubuntu Server no desktop como SO para experimentação.

Conforme relatado na literatura, o Ubuntu Server demonstrou desempenho satisfatório e baixo consumo de recursos. Contudo, havia dois equipamentos sendo utilizados no desenvolvimento do projeto: 1 -- \foreign{desktop} do próprio autor, sem qualquer restrição para troca e alteração de hardware e software; 2 -- \foreign{notebook} corporativo em que o sistema operacional não poderia ser alterado. Visando padronizar os ambientes de desenvolvimento, decidiu-se por padronizar o SO no Ubuntu Desktop em ambos os dispositivos.

No decorrer dessas alterações, a abordagem de monitoramento foi ampliada para contemplar não apenas servidores, mas também uma ampla variedade de dispositivos conectados à rede. Essa redefinição proporcionou maior versatilidade ao projeto, tornando desnecessária a aquisição de servidores físicos e possibilitando a utilização de contêineres tanto para a simulação de dispositivos quanto para a implementação da \foreign{stack} de monitoramento. Com isso, o processo de desenvolvimento tornou-se mais prático, ágil e modular, além de permitir um aproveitamento mais eficiente dos recursos computacionais disponíveis. Cabe destacar ainda que, ao adotar contêineres não só para testes, mas também para toda a infraestrutura de monitoramento, a discussão sobre a necessidade de hardware específico, como NUC ou Raspberry Pi, perdeu relevância neste contexto, já que a solução desenvolvida pode ser executada em qualquer máquina compatível com tecnologias de conteinerização.

%TODO: Em trabalhos futuros a análise de viabilidade de implementação em NUC ou Raspberry Pi
\begin{figure}[H]
\centering
\includegraphics[scale=0.97]{Imagens/chap03/v2_stack.pdf}
\caption{\foreign{Stack} reformulada.}
\label{fig:StackReformulada}
\end{figure}

\begin{figure}[H]
\centering
% \setlength{\abovecaptionskip}{-20pt}
\includegraphics[scale=0.97]{Imagens/chap03/v2_diagram.pdf}
\caption{Arquitetura reformulada.}
\label{fig:ArquiteturaReformulada}
\end{figure}


\subsection{Das versões iniciais}
\label{subsection:VersõesIniciais}

A virtualização desempenha um papel essencial neste trabalho. Além dos benefícios já mencionados, a conteinerização utilizando orquestradores como Docker Compose permite o versionamento de todo o trabalho desenvolvido em repositórios de controle de versão, como o Git. Esta abordagem rege todo o trabalho a seguir.

Desde o início do projeto, o Zabbix foi escolhido como principal ferramenta de monitoramento. Mesmo após a reformulação do escopo, o Zabbix permaneceu como a solução central, devido à sua capacidade de atender todos os requisitos do projeto: possui interface web, é de código aberto, é amplamente utilizado e consolidado no mercado, sendo compatível com diversos sistemas operacionais, arquiteturas e possuindo ampla documentação e ferramentas como agentes, proxies e servidores. Além disso, o Zabbix é altamente escalável, permitindo a adição de novos dispositivos e serviços monitorados de forma simples e eficiente.

Tendo o Zabbix como sistema de monitoramento central, posteriormente aco\-plaria-se a ele o Grafana. Apesar do Zabbix já possuir uma interface web com \foreign{dashboards} e gráficos, o Grafana oferece uma experiência de visualização mais rica e personalizável, sendo um complemento ideal.

A partir do repositório Docker oficial do Zabbix  \citep{zabbixdocker2025}, fez-se um \foreign{fork} para o repositório dedicado a este projeto. Essa abordagem permitiu a experimentação com os principais recursos conteinerizados da ferramenta, como servidor Zabbix, banco de dados MySQL, e agentes. O servidor foi inicialmente configurado com um \foreign{dashboard} básico, aproveitando o modelo fornecido pelo repositório Zabbix-Docker.

Após a execução bem-sucedida dos contêineres que compunham o sistema de monitoramento central, procedeu-se à instalação de um agente Zabbix no dispositivo móvel do autor para fins de teste. Foi possível realizar o \foreign{ping} do dispositivo a partir do servidor, sendo ambos --- o agente conteinerizado e o instalado no dispositivo móvel --- devidamente adicionados à lista de \foreign{hosts} do servidor.

É importante ressaltar que o Zabbix não dispõe de agentes oficiais para dispositivos móveis, mas disponibiliza recomendações de versões não oficiais desenvolvidas por terceiros para esse tipo de aplicação.

Durante essas implementações, foram identificados alguns pontos críticos que impactaram a continuidade do Zabbix como ferramenta principal de monitoramento do projeto: a necessidade de executar um contêiner adicional exclusivamente para o banco de dados (MySQL ou Postgres) eleva o custo computacional do ambiente; a interface gráfica do Zabbix apresentou limitações quanto à navegabilidade e usabilidade, dificultando a adaptação do autor; e o projeto Zabbix-Docker, por abranger múltiplas configurações, mostrou-se extenso e complexo, o que dificultou a leitura, compreensão e manutenção do código.

Além desses aspectos, o autor enfrentava simultaneamente a curva de aprendizado do Zabbix e do Docker, o que elevou significativamente o esforço necessário para a evolução do projeto. Tal cenário levou à decisão de abandonar o Zabbix, optando-se pela adoção do Prometheus nas etapas subsequentes deste trabalho.

\begin{figure}[H]
\centering
\includegraphics[scale=0.97]{Imagens/chap03/v3_stack.pdf}
\caption{\foreign{Stack} da versão-base do projeto.}
\label{fig:StackBase}
\end{figure}
\begin{figure}[H]
\centering
% \setlength{\abovecaptionskip}{-20pt}
\includegraphics[scale=0.98]{Imagens/chap03/v3_diagram.pdf}
\caption{Arquitetura da versão-base do projeto.}
\label{fig:ArquiteturaBase}
\end{figure}

\section{Descrição da Arquitetura}

Com o escopo e as ferramentas devidamente definidos, e tomando como referência a versão-base apresentada na seção anterior, esta seção descreve integralmente o trabalho desenvolvido: desde as simulações realizadas para obtenção de dados, passando pelos procedimentos de coleta e armazenamento, até a configuração dos mecanismos de visualização e alerta.

Nas subseções seguintes, são detalhados os principais blocos que compõem a arquitetura, seu funcionamento e as interações entre eles, culminando na apresentação da versão final da solução. Embora a ordem em que as subseções são apresentadas não siga estritamente a sequência cronológica do desenvolvimento, a estrutura adotada busca facilitar a compreensão do leitor, organizando o conteúdo de maneira progressiva conforme o fluxo natural de entrada, processamento e saída de dados.

% \begin{figure}[H]
% \centering
% \setlength{\abovecaptionskip}{-20pt}
% \includegraphics[width=\textwidth]{Imagens/chap03/v4_diagram.pdf}
% \caption{\textcolor{red}{Arquitetura.}}
% \label{fig:Arquiteturav1.0}
% \end{figure}

\subsection{Dispositivos virtuais}
\label{subsection:DispositivosVirtuais}

\begin{figure}[H]
\centering
% \setlength{\abovecaptionskip}{-20pt}
\includegraphics[scale=1]{Imagens/chap03/by-blocks/virtual_devices_diagram.pdf}
\caption{Dispositivos virtuais.}
\label{fig:DiagramaDispositivosVirtuais}
\end{figure}

Para a aquisição de dados, foram instanciados cinco contêineres Docker para simular uma rede de computadores. A fim de reproduzir um ambiente heterogêneo, cada contêiner recebeu restrições específicas de CPU e memória em sua configuração no Docker Compose. O Dockerfile, por sua vez, provisiona a distribuição Linux, \foreign{script} e software para geração de carga e o agente de monitoramento. Detalhamentos sobre essas ferramentas são apresentados nas subseções seguintes.

Denominados como dispositivos virtuais 1 a 5, eles foram configurados de acordo com as especificações resumidas na tabela a seguir:

\begin{table}[h]
\centering
\caption{Especificações dos dispositivos virtuais.}
\label{tab:EspecificaçõesDispositivosVirtuais}
\begin{tabular}{c c c c}
\hline
\textbf{Dispositivo Virtual} & \textbf{Distribuição Linux} & \textbf{CPU (\%)} & \textbf{Memória (MB)} \\
\hline
1 & Ubuntu 24 & 60 & 1024 \\
2 & Ubuntu 24 & 60 & 900 \\
3 & Ubuntu 24 & 70 & 800 \\
4 & Alpine 3.21 & 50 & 886 \\
5 & Alpine 3.21 & 60 & 750 \\
\hline
\end{tabular}
\end{table}

É importante destacar que o Docker, por padrão, considera 100\% de um núcleo como limite máximo de uso de CPU; logo, a atribuição de 0.6 no arquivo Compose indica que o contêiner está autorizado a utilizar até 60\% da capacidade de um único núcleo.

{\color{red}
Os valores atribuídos às limitações de memória foram definidos a partir de cálculos estimativos, considerando a carga potencial gerada pelo \foreign{script} de geração de carga, acrescida do consumo médio do sistema operacional e do agente Telegraf. Durante os testes iniciais, esses parâmetros foram ajustados empiricamente, de modo a assegurar a operação estável dos contêineres dentro dos limites estabelecidos. Detalhes adicionais sobre esse processo estão apresentados na Seção \ref{subsection:TestesSaturacao}.
}
% O código fonte do Docker Compose e dos Dockerfiles empregados na criação desses dispositivos virtuais encontra-se disponível no Apêndice \ref{apendiceA}.

\subsection{Agentes}
\label{subsection:Agentes}

Placeholder
% {\color{red}
% Quanto aos agentes, foram consideradas duas abordagens distintas: a primeira, a adição de um novo contêiner ao à rede Docker, executando o serviço do cAdvisor, uma ferramenta amplamente utilizada no monitoramento de contêineres, como apresentado em \ref{subsection:cAdvisor}. No entanto, o cAdvisor 

% %Aqui eu preciso falar dos agentes que só liam dados do host e nao dos containers, explicar que eu quis usar agentes integrados (embedded) nos dispositivos virtuais, e basicamente justificar pq eu larguei os outros e fiquei com telegraf.
% }


% {1 - Atualizei a estratégia de coleta de métricas do Docker: agora estou utilizando o plugin inputs.docker diretamente no telegraf.conf do host. Dessa forma, o Telegraf coleta métricas diretamente do socket do Docker daemon (/var/run/docker.sock), garantindo maior precisão, já que os dados vêm da mesma fonte usada pelo próprio Docker para monitoramento. Com isso, obtenho métricas essenciais como uso de CPU, memória, disco (I/O) e rede. Esses dados podem ser usados para comparações qualitativas com as métricas coletadas por Telegrafs rodando dentro dos containers.

% Mas fica a reflexão: faz sentido comparar métricas entre dispositivos virtuais? Quando o objetivo era comparar máquinas físicas versus containers, essa comparação qualitativa era justificável. Porém, ao comparar container versus container, será que essa análise ainda se sustenta? Há valor prático em comparar dispositivos virtuais entre si nesses termos?

% --> Realmente não faz sentido, logo não é necessário fazer esse levantamento.
% }

\begin{figure}[H]
\centering
% \setlength{\abovecaptionskip}{-20pt}
\includegraphics[scale=1]{Imagens/chap03/by-blocks/agents_diagram.pdf}
\caption{Agentes Telegraf.}
\label{fig:DiagramaAgentes}
\end{figure}


\subsection{Prometheus e TSDB}
\label{subsection:PrometheusTSDB}
Para implementar o sistema de monitoramento central, o serviço Prometheus foi adicionado ao Docker Compose. Essa inclusão trouxe não apenas o ferramental para coleta das métricas expostas pelos agentes Telegraf, mas também o banco de dados de séries temporais (TSDB), responsável pelo armazenamento permanente dessas métricas. Como o TSDB é parte integrante do Prometheus, não foi necessária nenhuma configuração adicional para sua integração, o que não só simplificou significativamente a implementação como também reduziu o custo e otimizou o desempenho do sistema.

No Compose, além das diretivas comuns de configuração de contêineres --- como mapeamento de portas, redes e volumes --- destacam-se as diretivas \verb|extra_hosts| e \verb|command|. A primeira permite que o contêiner do Prometheus consiga acessar o host da máquina onde está sendo executado, configuração fundamental para coleta das métricas do agente Telegraf em execução no host. A segunda, por sua vez, configura o tempo de retenção dos dados no TSDB para 15 dias, utilizando o comando \verb|--storage.tsdb.retention.time=15d|.

Já no arquivo \verb|prometheus.yml|, são definidas as regras globais de \textcolor{red}{\foreign{scraping} (raspagem) de dados}, os direcionamentos para o arquivo de regras e para o serviço de alertas (Alertmanager -- a ser detalhado posteriormente), além das configurações dos \foreign{endpoints} do Telegraf para coleta de dados. Ressalta-se que, em \verb|global|, o parâmetro \verb|scrape_interval| é definido como 5 segundos, enquanto o \verb|scrape_timeout| é ajustado para 4 segundos. Essa configuração proporciona uma coleta de alta frequência, permitindo maior granularidade dos dados. Ao mesmo tempo, o limite de 4 segundos para o tempo de requisição evita bloqueios prolongados: caso uma coleta exceda esse tempo, o Prometheus considera a tentativa como falha e parte para uma nova coleta no próximo intervalo, o que contribui para a prevenção de congestionamentos no sistema.

Além disso, há uma importante sincronia entre os intervalos de \textcolor{red}{\foreign{scraping}} do Prometheus e os de coleta do Telegraf --- ambos configurados para 5 segundos. Essa equivalência garante que as métricas coletadas estejam sempre atualizadas e alinhadas entre as duas ferramentas.

\begin{figure}[H]
\centering
% \setlength{\abovecaptionskip}{-20pt}
\includegraphics[scale=1]{Imagens/chap03/by-blocks/prometheus_diagram.pdf}
\caption{Prometheus e TSDB.}
\label{fig:DiagramaPrometheusTSDB}
\end{figure}

\subsection{Grafana}
\label{subsection:GrafanaVisualizacao}

Como mencionado no Capítulo \ref{chap2}, o Prometheus Expression Browser oferece uma interface web básica para visualização de métricas, porém essa interface apresenta limitações e não atende às demandas para visualizações avançadas. Por esse motivo, o próximo passo do projeto foi integrar o Grafana.

Assim como nos demais serviços, a inclusão do Grafana ao Docker Compose envolveu apenas a definição das configurações básicas. Neste caso, é importante destacar a utilização da diretiva \verb|volumes|, que aponta para dois caminhos específicos: \verb|grafana/dashboard-json| e \verb|grafana/provisioning|. O primeiro armazena os arquivos JSON dos \foreign{dashboards}, que serão importados automaticamente pelo Grafana durante a inicialização. O segundo contém as configurações de provisionamento, usadas para automatizar a integração de fontes de dados e \foreign{dashboards}. Esse mecanismo é essencial para a automação do projeto, pois garante que todas as configurações necessárias --- tanto das integrações com fontes de dados, como o Prometheus, quanto dos \foreign{dashboards} --- sejam carregadas automaticamente ao iniciar o contêiner, dispensando procedimentos manuais pela interface web.

Adicionalmente, a imagem oficial do Grafana utilizada no projeto já inclui o banco de dados SQLite, responsável por armazenar as configurações realizadas por meio da interface gráfica. As configurações aplicadas via provisionamento são acessíveis em modo \foreign{read-only} na interface e não podem ser modificadas manualmente. Já as alterações feitas diretamente pela interface gráfica são gravadas no SQLite, mas não são replicadas para os arquivos de provisionamento. Em situações de conflito entre as configurações do provisionamento e as definidas pela interface gráfica, prevalecem aquelas estabelecidas pelo provisionamento.

\begin{figure}[H]
\centering
% \setlength{\abovecaptionskip}{-20pt}
\includegraphics[scale=1]{Imagens/chap03/by-blocks/grafana_diagram.pdf}
\caption{Grafana}
\label{fig:DiagramaGrafana}
\end{figure}


\subsection{Testes de saturação}
\label{subsection:TestesSaturacao}

Os dispositivos virtuais, por si só, não produzem dados suficientes para uma análise qualitativa ou quantitativa eficaz do sistema de monitoramento. Por esse motivo, foram implementados testes de saturação, consistindo na geração de carga adicional sobre os dispositivos virtuais para simular cenários de uso intenso e coletar as métricas correspondentes.

Inicialmente, a introdução de carga foi realizada exclusivamente com o stress-ng, integrado aos Dockerfiles dos dispositivos virtuais. Posteriormente, desenvolveu-se um \foreign{script} Bash que executa o stress-ng com diferentes parâmetros, simulando variados tipos de carga (CPU, memória, I/O, etc.) e níveis de intensidade. Esse \foreign{script} é executado em segundo plano, simultaneamente à execução do agente Telegraf, garantindo a geração contínua de carga durante a coleta das métricas.

Apesar do bom desempenho na geração de carga computacional, o stress-ng mostrou-se insuficiente para simular tráfego de rede de forma realista. Para superar essa limitação, foi incorporado o iPerf3 ao projeto. Como esta ferramenta opera no modelo cliente-servidor e não permite a atuação simultânea nessas funções, foi criado um contêiner dedicado para funcionar como servidor iPerf3. Em paralelo, cada dispositivo virtual passou a contar com uma instância do iPerf3 configurada como cliente. Dessa maneira, tornou-se possível gerar tráfego de rede entre os dispositivos virtuais e o servidor iPerf3, aprimorando a simulação de carga sobre o ambiente de rede.

{\color{red}

\textcolor{blue}{Rascunho:
[FALAR SOBRE AS ESPECIFICAÇOES DOS TESTES (PRIMEIRO UM DE CADA VEZ, DEPOIS TODOS JUNTOS, CARGAS, INTENSIDADES, DURAÇOES, ETC.)]}


}

\begin{figure}[H]
\centering
% \setlength{\abovecaptionskip}{-20pt}
\includegraphics[scale=1]{Imagens/chap03/by-blocks/saturation_diagram.pdf}
\caption{Ferramentas de saturação.}
\label{fig:DiagramaSaturacao}
\end{figure}


{\color{red}

\subsection{Desafios na Integração de Dispositivos Móveis}
\label{subsection:DesafiosDispositivosMoveis}

Diversas tentativas de incluir dispositivos móveis na rede de monitoramento foram realizadas ao longo do desenvolvimento, com o intuito de ampliar a diversidade dos equipamentos monitorados. No entanto, obstáculos técnicos --- como a ausência de suporte oficial para Android e restrições de permissão do sistema operacional (incluindo limitações de segurança e necessidade de acesso \foreign{root}) --- inviabilizaram essa integração. Embora não tenham resultado em sucesso, essas experiências contribuíram para um melhor entendimento das limitações e desafios envolvidos no monitoramento de dispositivos móveis.

Destaca-se que, em uma das tentativas, foi possível realizar com êxito um \foreign{ping} ao dispositivo móvel do autor, equipado com uma versão não oficial do Zabbix Agent recomendada pela própria Zabbix \citep{unofficialzabbixagent2025}, via contêiner do servidor Zabbix (anterior à adoção do Prometheus). Ainda assim, as dificuldades anteriormente expostas impediram o avanço dessa abordagem.

Considerou-se também a utilização do \foreign{agentless monitoring} (monitoramento sem agentes) por meio do SNMP, mas essa alternativa foi descartada devido à complexidade adicional e às limitações do protocolo, como a necessidade de pré-configuração nos dispositivos e a restrição quanto ao tipo e à quantidade de métricas acessíveis.

Também estudou-se a simulação de dispositivos móveis por meio de emuladores Android, alternativa igualmente abandonada devido ao aumento da complexidade do projeto.
}

\textcolor{blue}{Dúvida: Por ter mencionado o Android nesse texto, sou obrigado a acrescentá-lo lá no embasamento teórico no capítulo 2?}



\section{Infraestrutura}

{\color{red}

Ao longo do projeto, dois equipamentos estiveram à disponibilidade do autor: um desktop pessoal e um \foreign{notebook} corporativo, conforme mencionado em \ref{subsection:EscopoReformulado}. As especificações desses equipamentos estão detalhadas na Tabela \ref{tab:available-hardware}.
}
\begin{table}[H]
\centering
\caption{Especificações de hardware dos equipamentos disponíveis}
\label{tab:available-hardware}
\begin{tabular}{lcc}
\toprule
\textbf{Componente} & \textbf{Desktop} & \textbf{Notebook} \\
\midrule
CPU   & Intel Core i5-7600   & Intel Core i7-8565U \\
RAM   & 16GB                 & 32GB                \\
Disco & 500GB (SSD)            & 250GB (SSD)          \\
SO & Ubuntu Desktop 24 & Ubuntu Desktop 20 \\
\bottomrule
\end{tabular}
\end{table}

{\color{red}

As práticas adotadas ao longo deste projeto possibilitaram um desenvolvimento multi-máquina prático e eficiente. Cada equipamento implementou o projeto por meio de clones locais do repositório e executou os mesmos comandos Docker Compose para iniciar toda a infraestrutura de monitoramento. A possibilidade de rodar o mesmo projeto em diferentes máquinas, sem necessidade de instalações manuais ou configurações complexas, destacou-se como uma das principais vantagens do desenvolvimento.

Entretanto, a partir de determinado momento, o \foreign{desktop} começou a apresentar instabilidades e travamentos de origem desconhecida. Esses problemas tiveram impacto negativo na coleta de dados e, consequentemente, nas visualizações, ocasionando \foreign{gaps} nas curvas dos gráficos e perda de informações. Diante desse contexto, decidiu-se consolidar o \foreign{notebook} como equipamento principal para o desenvolvimento do projeto, descartando o uso do \foreign{desktop}.
}
\section{Discussão sobre as métricas}

Placeholder

% foram feitas tentativas de utilização de ferramentas de \foreign{chaos-engineering} como Pumba e ChaosBlade e até mesmo tentativas de manipulação direta de iptables para gerar tráfego de rede, mas essas ferramentas não se mostraram adequadas para este trabalho.


% Por debaixo dos panos, ferramentas de \foreign{chaos testing} utilizam para testes de rede o módulo \verb|netem| do sistema de controle de tráfego \verb|tc| do Linux. Esse sistema opera na camada de controle de tráfego (\verb|qdisc|). Enquanto isso, o \foreign{plugin} \verb|inputs.net| do Telegraf a partir da leitura do \verb|procfs/net/dev|, que se encontra na camada de interface de rede. Ou seja, 


\section{Aplicação de Monitoramento}

Placeholder

Falar dos alertas e notificações
Discorrer sobre o dashboard em si

% \textcolor{blue}{Dúvida: Como combinamos de contar a experiencia com o SQLitexPostgres e tambem a alteração para Alertmanager na seção 3.5 (alertas e dashboards), confesso que não sei mais o que colocar aqui... talvez matemos essa subseção 3.2.6?}

% Uma dúvida que tive hoje nesta seção: nós combinamos de comentar sobre dashboards e alertas na seção 3.5, porém isso implica que eu não incluiria os respectivos blocos de dashboards e alertas (e Alertmanager) no diagrama neste momento. Isso implica que a figura do diagrama final da arquitetura só apareceria no fim da seção 3.6. Como proceder?

% Aqui a finalizo a seção contando a historia do SQLite x Postgres e eventualmente chegando na decisão de descartar ambos e ficar apenas com o TSDB.
% Tambem relato da minha experiencia de formatar a maquina toda e a facilidade em subir todo o projeto (salve a configuração pontual do permissionamento de escrita do grafana e da chave do repositorio do telegraf)
% Após isso eu fecho a seção com o diagrama da arquitetura final definitiva. 
